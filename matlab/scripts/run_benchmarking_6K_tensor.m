% This code loads the '6K' tensor data (i.e. the tensor with over 6K drugs
% generated by simply filtering the CD signatures to p<0.1. It loads a
% version of the data that Jingshu had split up into train, validation and
% test, and where the validation set was further split into a dense part
% (i.e. anything in the domain of the 611-drug tensor studied previously),
% and a sparse part. It then completes the data using the four baselines
% from the initial paper (PSB) and generates predictions on the validation
% subset.

dir = '~/Desktop/Box/Rachel/Thesis/Code/thesis/data/expr/drug/from_jingshu/';

% Load training data
train.data = load([dir 'T_d6K_train.txt']);
train.drugIdx = train.data(:,1) + 1;
train.cellIdx = train.data(:,2) + 1;
train.data = train.data(:,3:end);

% And test (actually validation) data
test.data = load([dir 'T_d6K_val.txt']);
test.drugIdx = test.data(:,1) + 1;
test.cellIdx = test.data(:,2) + 1;
test.data = test.data(:,3:end);

%%% And also identify the sparse and dense subsets of the validation set
% sparse
test_sp.data = load([dir 'T_d6K_val_sp.txt']);
test_sp.drugIdx = test_sp.data(:,1) + 1;
test_sp.cellIdx = test_sp.data(:,2) + 1;

% dense
test_dns.data = load([dir 'T_d6K_val_dense.txt']);
test_dns.drugIdx = test_dns.data(:,1) + 1;
test_dns.cellIdx = test_dns.data(:,2) + 1;

% sanity check that the dense subset is restricted to roughly the first 600
% drugs and 11 cell types (differences arise because the two tensors were sorted
% based on overall signature availability in each tensor)
max(unique(test_dns.cellIdx)) % 12
max(unique(test_dns.drugIdx)) % 947

% Convert to single precision for computational efficiency
train.data = single(train.data);
test.data = single(test.data);

% Fold training data into a tensor
T = nan([6551, 978, 71]);
for i = 1:size(train.data, 1)
    T(train.drugIdx(i),:,train.cellIdx(i)) = train.data(i,:);
end

%%% Complete tensor using four different methods and extract predictions on
%%% validation set
models = {'mean', 'mean2','knnd','fa_lrtc'};
nModels = length(models);
time = zeros(1, nModels);
pred = cell(nModels, 1);
for m = 1:nModels
    disp(models{m})
    args = GetArgs(models{m}, [], [], size(T));
    [T_pred, time(m)] = CompleteTensor(T, models{m}, args);
    disp(sprintf('Finished %s', models{m}))
    pred{m} = nan(size(test.data, 1), 978);
    for i = 1:size(test.data, 1)
        pred{m}(i,:) = T_pred(test.drugIdx(i),:,test.cellIdx(i));
    end
    clear T_pred
end

%%% Then compute the row mapping between the full test set and the two
%%% subsets (dense and sparse)

% first on the dense set
n_dns = length(test_dns.drugIdx);
dns_idx = nan(n_dns);
for i = 1:n_dns
    idx1 = find(test.drugIdx == test_dns.drugIdx(i));
    idx2 = find(test.cellIdx == test_dns.cellIdx(i));
    idx = intersect(idx1, idx2);
    assert(length(idx) == 1);
    assert(test_dns.data(i,:) == test.data(idx,:));
    dns_idx(i) = idx;
end

% and also on the sparse set
n_sp = length(test_sp.drugIdx);
sp_idx = nan(n_sp);
for i = 1:n_sp
    idx1 = find(test.drugIdx == test_sp.drugIdx(i));
    idx2 = find(test.cellIdx == test_sp.cellIdx(i));
    idx = intersect(idx1, idx2);
    assert(length(idx) == 1);
    assert(test_sp.data(i,:) == test.data(idx,:));
    sp_idx(i) = idx;
end

% Finally, run a few sanity checks on the mappings
assert(isempty(find(isnan(dns_idx))));
assert(isempty(find(isnan(sp_idx))));
assert(isempty(intersect(dns_idx, sp_idx)));
assert(sort(union(dns_idx, sp_idx)) == 1:size(test.data, 1));


% Compute sqrt err rate on predictions for each of the four methods
for m = 1:nModels
   err{m}.full = ComputeErrorRate(test.data, pred{m});
   err{m}.sp = ComputeErrorRate(test_sp.data, pred{m}(sp_idx,:);
   err{m}.dns = ComputeErrorRate(test_dns.data, pred{m}(dns_idx,:));
end
